{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#overview","title":"Overview","text":"<p>The AI Verify Toolkit is an open-source, extensible toolkit that validates the performance of AI systems against a set of 11 internationally recognised AI ethics principles through process checks and technical tests. AI Verify can be used by companies for self-assessment, or by independent testers to verify AI models against the AI Verify Testing Framework. It supports the technical assessment of supervised learning models trained on most tabular and image datasets for binary / multiclass classification and regression models.</p>"},{"location":"#how-does-the-toolkit-work","title":"How does the toolkit work?","text":"<p>The toolkit operates in a report-oriented workflow where you start with your \u2018end-goal\u2019 in mind. Through the customisable report canvas, design page-by-page using report widgets what you want your report to consist of. This will determine the technical tests and process checks needed to be done. AI Verify will then streamline your workflow to collect only the relevant files, test arguments and user inputs needed to run the tests and generate your customized report.</p> <p>To help companies align their reports with the AI Verify Testing Framework, the toolkit also comes with a set of report templates, which pre-defines the report layout, technical tests and process checks needed. Checkout the sample report generated using the AI Verify report template here.</p> <p>To extend the suite of existing testing functionalities, you can install plugins built by the AI Verify Foundation or third parties.</p>"},{"location":"#supported-environments","title":"Supported environments","text":"<p>The AI Verify Toolkit can be deployed in 2 ways.</p> <ul> <li>Build and run from Dockerfile: This method is best for a quick installation and start-up.</li> <li>Build and run from source code: This method allows for more flexibility.</li> </ul>"},{"location":"#_1","title":"Introduction","text":"<p>For a more detailed breakdown of how AI Verify works, check out How It Works</p> <p>To jump directly into the tool, Download the Quick Start Guide here. </p> <p>If you are looking to contribute to the AI Verify Toolkit, Visit the Developer Documentation here.</p>"},{"location":"getting-started/accessing-ai-model/","title":"Accessing AI Model","text":"<p>The AI Verify Toolkit supports two modes of accessing the AI models to be tested.</p> Mode of Access Framework Libraries Supported Dataset Type Upload AI Model LightGBM, Scikit-learn, Tensorflow, XGBoost Tabular Only Upload Pipline Scikit-learn pipeline Tabular, image <p>The list of datatype formats supported are as follows:</p> Dataset Type Formats Supported Tabular Pandas, Delimiter-separated Values (comma, tab, semicolon, pipe, space, colon) Image .jpeg, .jpg, .png"},{"location":"getting-started/accessing-ai-model/#upload-ai-model","title":"Upload AI Model","text":""},{"location":"getting-started/accessing-ai-model/#upload-pipeline","title":"Upload Pipeline","text":"<p>If your dataset requires pre-processing before being fed into the prediction model, you can upload the pre-processing functions together with your model as a pipeline folder.</p> <p></p> <p>Currently, the toolkit supports a limited set of models. Check out the full list of framework and algorithm types supported.</p>"},{"location":"getting-started/docker-setup/","title":"Build and run from Dockerfile","text":""},{"location":"getting-started/docker-setup/#requirements","title":"Requirements","text":"<p>To run AI Verify, these are the recommended requirements to run AI Verify on a local computer.</p>"},{"location":"getting-started/docker-setup/#system","title":"System","text":"Recommended Requirements Operating System Ubuntu 22.04 CPU* At least 4 cores (Please note that this varies based on the complexity of the model tested.) Memory At least 16GB Free Space At least 6GB of available space GPU* Depends on the model tested"},{"location":"getting-started/docker-setup/#software","title":"Software","text":"Software Recommended Version Docker ##TO BE UPDATED NodeJs v18.x Python v3.10 Git v2.40.0"},{"location":"getting-started/docker-setup/#clone-and-build-ai-verify","title":"Clone and Build AI Verify","text":"<p>Note</p> <p>If you have installed AI Verify before, it is recommended to start the containiner with a clean state. Run the following commands to clean up existing data and files:    <pre><code>bash docker-start.sh --reset\n</code></pre></p> <ol> <li> <p>Clone AI Verify to your local computer. Open a terminal: <pre><code>git clone https://github.com/IMDA-BTG/aiverify.git aiverify\n</code></pre></p> </li> <li> <p>Type the following commands to build the docker image. <pre><code>cd aiverify/build\nbash docker-build.sh\n</code></pre></p> </li> <li> <p>Click on the \"Images\" tab in Docker Desktop, you should see <code>ai-verify</code> running in Docker:  </p> </li> </ol> <p>Warning</p> <p>It will take a while to build the docker image as there are many dependencies and modules required for the environment.   </p>"},{"location":"getting-started/docker-setup/#run-ai-verify","title":"Run AI Verify","text":"<ol> <li> <p>Start AI Verify using our start script <pre><code>bash docker-start.sh\n</code></pre></p> </li> <li> <p>Type http://localhost:3000 into your browser's address bar.     </p> </li> </ol>"},{"location":"getting-started/preparation-of-input-files/","title":"Preparation of Input Files","text":"<p>To run the technical tests, you need to prepare input fles. To find out the input files needed by a specific technical test, check out the Plugin Documentation here.</p> Input File/ Folder Description AI Model or Pipeline The prediction model to be tested. A pipeline may include both data pre-processing and the prediction model. Testing Dataset Any dataset to be used for testing. Ground Truth Dataset (For tabular) A dataset that contains the ground truth. The testing dataset can be used if it contains the ground truth. Ground Truth Dataset/ Annotated Ground Truth Dataset (For image) A dataset containing the image file names and ground truth. Background Dataset The background dataset should be representative of the dataset\u2019s population. The testing dataset can be used if it is representative of the dataset\u2019s population. <p>All input files should be smaller than 4GB and be serialized by pickle or joblib (except for image datasets and TensorFlow models).</p> <p>For reference on how the model and dataset files can be prepared, check out the sample python notebook here. </p>"},{"location":"getting-started/quick-start-guide/","title":"Quick Start Guide","text":"<p>Info</p> <p>Users are recommended to follow the Docker Setup installation.</p>"},{"location":"getting-started/source-code-setup/","title":"Build and run from source code","text":"<p>Info</p> <p>   This method of installation are recommended for Advanced Users.</p>"},{"location":"getting-started/source-code-setup/#requirements","title":"Requirements","text":"<p>To run AI Verify, these are the minimum requirements to run AI Verify on a local computer.</p>"},{"location":"getting-started/source-code-setup/#system","title":"System","text":"Minimum Requirements Operating System Ubuntu 22.04 CPU* At least 4 cores (Please note that this varies based on the complexity of the model tested.) Memory At least 16GB Free Space At least 6GB of available space GPU* Depends on the model tested"},{"location":"getting-started/source-code-setup/#software-requirements","title":"Software Requirements","text":"Software Minimum Version NodeJs v18.x Python v3.10 Git v2.40 MongoDB* v6.x <p>Warning</p> <p>If you already have MongoDB, you are required to execute these steps.</p> <pre><code>$ mongosh\n\naiverify = db.getSiblingDB('aiverify')\naiverify.createUser({\nuser: 'aiverify',\n   pwd: 'aiverify',\n   roles: [{ role: 'readWrite', db: 'aiverify' }],\n   });\n</code></pre>"},{"location":"getting-started/source-code-setup/#install-ai-verify","title":"Install AI Verify","text":"<ol> <li>Create a folder where you intend to install AI Verify. <pre><code>mkdir ai-verify-env\n</code></pre></li> <li>Install the <code>setup-aiverify-dev.sh</code> file from our GitHub release page and copy the file into the folder created above.</li> <li>Open a Terminal in the directory</li> <li>Execute the script by executing the following code.    <pre><code>cd ai-verify-env #navigate to the folder created\nbash setup-aiverify-dev.sh\n</code></pre></li> <li>Wait for the set up to finish, do observe the logs for errors.</li> </ol>"},{"location":"getting-started/source-code-setup/#start-ai-verify","title":"Start AI Verify","text":""},{"location":"getting-started/source-code-setup/#redis-and-mongodb","title":"Redis and MongoDB","text":"<p>Redis and MongoDB are installed as system services, and should already be running. Execute the following code to check that the services are running.</p> <pre><code>sudo systemctl status redis\nsudo systemctl status mongod\n</code></pre> <p>If they are not running, execute these code to run them:</p> <pre><code>sudo systemctl start redis\nsudo systemctl start mongod\n</code></pre>"},{"location":"getting-started/source-code-setup/#ai-verify-modules","title":"AI Verify Modules","text":"<p>AI Verify modules are configured as system services. To ensure AI Verify modules are running, following the instruction below to run each of the required services:</p>"},{"location":"getting-started/source-code-setup/#test-engine-app","title":"test-engine-app","text":"<pre><code># Open a new terminal window #\ncd ai-verify-env/aiverify\nsource venv/bin/activate\ncd test-engine-app\npython3 -m test_engine_app\n</code></pre>"},{"location":"getting-started/source-code-setup/#ai-verify-apigw","title":"ai-verify-apigw","text":"<pre><code># Open a new terminal window #\ncd ai-verify-env/aiverify/ai-verify-apigw\nnode app.mjs\n</code></pre>"},{"location":"getting-started/source-code-setup/#ai-verify-portal","title":"ai-verify-portal","text":"<pre><code># Open a new terminal window #\ncd ai-verify-env/aiverify/ai-verify-portal\nnpm run start\n</code></pre>"},{"location":"getting-started/source-code-setup/#running-ai-verify","title":"Running AI Verify","text":"<ol> <li>Once all the services are up and running, open your browser and type localhost:3000 in the address bar. You should see the AI Verify home page appears and will be able to access its functions.    </li> </ol>"},{"location":"introduction/extensibility/","title":"Extensibility","text":"<p>In the fast-evolving world of AI today, a concerted global community effort is required to keep our AI governance and testing capabilities on par with the state of the art in AI development and use. AI Verify is developed as an open-source, extensible tool to allow third-party developers and researchers to add to a global body of work. One way in which these contributors can extend the testing functionalities of AI Verify is through building plugins.</p>"},{"location":"introduction/extensibility/#plugins","title":"Plugins","text":"<p>Plugins are the basis of testing functionalities in the AI Verify Toolkit. The toolkit comes with a set of pre-installed plugins that contain the technical tests and process checks required by the AI Verify Testing Framework. To extend this basic set of testing functionalities, you can install plugins built by the AI Verify Foundation and other third-party developers.</p> <p>Plugins can consist of algorithms, widgets, input blocks, and report templates and they work together as such: </p> Plugin component Description Algorithms AI model testing algorithms Input Blocks User input capture fields Widgets Report components to visualize data/ test results Templates Defines customised report layouts <p>To understand more about the pre-installed plugins, check out the Plugin Documentation here</p> <p>For developers interested to build plugins, check out the Plugin Developer Guide here </p>"},{"location":"introduction/how-it-works/","title":"How It Works","text":""},{"location":"introduction/how-it-works/#workflow","title":"Workflow","text":"<p>The AI Verify Toolkit operates in a report-oriented workflow where you start with your \u2018end-goal\u2019 in mind. Through the customisable report canvas, design page-by-page using report widgets what you want your report to consist of. Be it a bar chart for fairness metrics, or a pie chart summarising how the AI system has aligned with the testable criteria for Safety. This will determine the technical tests and process checks needed to be done. AI Verify will then streamline your workflow to collect only the relevant files, test arguments and user inputs needed to run the tests and generate your customized report.</p> <p>To help companies align their reports with the AI Verify framework, the toolkit also comes with a set of report templates, which pre-defines the report layout, technical tests and process checks needed. Checkout the sample report generated using the report templates on our AI Verify Foundation website. </p>"},{"location":"introduction/how-it-works/#technical-test","title":"Technical Test","text":"<p>The AI Verify Toolkit conducts black-box testing on AI models by ingesting the AI model to be tested in the form of a serialized model file/folder. Depending on the test to run, various dataset files and test arguments will also be needed.</p> <p>The AI Verify report templates contains technical tests that covers 3 principles:</p> <ul> <li>Fairness: Fairness Metrics Toolbox for Classification &amp; Fairness Metrics Toolbox for Regression</li> <li>Robustness: Robustness Toolbox</li> <li>Explainability (Global): SHAP Toolbox</li> </ul> <p>There are also two technical tests for explainability that are not included in the AI Verify report templates:</p> <ul> <li>Accumulated Local Effect</li> <li>Partial Dependence Plot</li> </ul>"},{"location":"introduction/how-it-works/#process-checks","title":"Process Checks","text":"<p>The AI Verify Toolkit supports the AI Verify Testing Framework by providing an integrated interface that helps you to track the completion progress of the 85 testable criteria over the 11 Process Checklists, and generating a summary of how the AI system aligns with the AI Verify Testing Framework.</p> <p>Download a copy of the AI Verify Testing Framework</p>"},{"location":"introduction/how-it-works/#relevant-links","title":"Relevant Links","text":"<ul> <li>Accessing the AI model to be tested</li> <li>Preparation of Input Files</li> <li>List of Supported AI Framework and Datatypes</li> </ul>"},{"location":"others/compatibility/","title":"Compatibility Table","text":""},{"location":"others/compatibility/#ai-framework-and-model-types","title":"AI Framework and Model Types","text":"<p>In this table, we list the supported AI framework and algorithms. </p> Framework Version Algorithm Model Type scikit-learn 1.2.2 Binary Classification Logistic Regression Decision Tree Random Forest Gradient Boosting Classifier Perceptron Bagging Classifier Multiclass Classification Logistic Regression Decision Tree Random Forest Gradient Boosting Classifier Perceptron Bagging Classifier Regression Linear Regression Extra Tree Regressor Gradient Boosting Regressor Random Forest Regression Tensorflow 2.12.0 Binary Classification Keras Sequential Multiclass Classification Keras Sequential Regression Keras Sequential XGBoost 1.7.5 Binary Classification XGB Classifier XGB Booster Multiclass Classifcation XGB Classifier Regression XGB Regressor LightGBM 3.3.5 Binary Classification LGBM Classifier"},{"location":"others/compatibility/#data-serialisers","title":"Data Serialisers","text":"Library Version pickle Version is based on the pickle installed in your environment joblib 1.20 <p>*If your datasets and models are serialised using other version, please modify your environment accordingly.</p>"},{"location":"others/faq/","title":"FAQs","text":""},{"location":"others/faq/#installation","title":"Installation","text":"Installation Does the toolkit run in Jupyter notebooks? No, the toolkit is a standalone toolkit. Users can use the user interface provided to run the testing workflow. Does the toolkit run on Apple\u2019s M1/M2 chips? No, testing is currently not supported on Apple\u2019s M1/M2 chips as they do not support the Tensorflow library used by the toolkit."},{"location":"others/faq/#models","title":"Models","text":"Models What is the list of models supported by the toolkit? Currently, the Upload AI Model method supports limited binary classification and regression models trained by algorithms found in scikit-learn, Tensorflow, XGBoost and LightGBM. How should I prepare the model to be uploaded? Answer to question Is there any limit to the model file size? The maximum size for uploading of the model file and test dataset file is 2 GB each. It is recommended to use a smaller test dataset (i.e., smaller than 2GB) as a large file may take significantly longer to complete. The time taken to run the test will also be affected by the number of features."},{"location":"others/faq/#datasets","title":"Datasets","text":"Datasets Does the toolkit support categorical values in string format? The toolkit currently does not support columns with categorical values in string format. The toolkit requires these columns to be encoded to numeric values before running any technical tests. What pandas version is currently supported by the toolkit? The toolkit is able to support pandas version 1.3.5. Currently, with the latest pandas version 1.4.3, the toolkit is unable to unpickle and read files for testing. Can the toolkit support .csv or .xlsx inputs? Answer to question How should I prepare the dataset to be uploaded? Answer to question What is considered a good input size for the test data? Answer to question"},{"location":"others/faq/#results-reports","title":"Results &amp; Reports","text":"Results &amp; Reports How to view files from the downloaded test results and logs? Answer to question"},{"location":"trouble-shooting/troubleshooting/","title":"Troubleshooting","text":"<p>Here are some common issues and solutions for problems that users may encounter while using the software.</p> Category Issue Subscription Setup &amp; Installation Systemctl command is not available if you are using WSL. We reccomend running the application in a virtual machine. Else, try this fix here: https://devblogs.microsoft.com/commandline/systemd-support-is-now-available-in-wsl/ Setup &amp; Installation MAC and Linux users might encounter issues connecting to mongoDB. Go to lib/mongodb.mjs and change <code>const DB_URI = process.env.DB_URI || 'mongodb://aiverify:aiverify@localhost:27017/aiverify';</code> to <code>const DB_URI = process.env.DB_URI || 'mongodb://aiverify:aiverify@127.0.0.1:27017/aiverify';</code> Test Run Test run fails with error: <code>There was an error getting algorithm instance (not found)</code> You might have some missing algorithm dependencies. Go to the 'Algorithm' tab for this plugin in the Plugin Manager Page and install any missing required packages."},{"location":"user-interface-features/asset-manager-page/","title":"Asset Manager Page","text":"<p>You can access the Models &amp; Data management pages via the home page the menu icon at the header.</p> <p></p> <p>Assets in the toolkit are categorised into two main categories: Datasets and AI Models.</p> <p>Datasets and Models uploaded onto the toolkit are stored in two different directories and subjected to different file validation processes.</p> <p>In the assets page, click on \u2018New Dataset +\u2019 or \u2018New AI Model +\u2019 as shortcuts to the respective file upload pages or select one of the folders to view files previously uploaded.</p>"},{"location":"user-interface-features/asset-manager-page/#existing-assets","title":"Existing Assets","text":"<p>The datasets page lists the files that have been uploaded as datasets and the models page lists the files that have been uploaded as models. Click on each row to view its file details.</p>"},{"location":"user-interface-features/asset-manager-page/#filters","title":"Filters","text":"<p>You can search for files by its file name and sort the lists by clicking on each column header. You can also sort the files by these filters:</p> <p>Dataset filters:</p> Filter Description File Show datasets uploaded as a single file. Folder Show datasets uploaded as a folder. <p>Model filters:</p> Filter Description Classification how models where the user indicated \u2018Classification\u2019 as its model type. Regression Show models where the user indicated \u2018Regression\u2019 as its model type. File Show models uploaded as a single file. Folder Show models uploaded as a folder. Pipeline Show models uploaded as a pipeline. <p>For each file uploaded onto the toolkit, the validation process extracts various details such as the model format, shape of the dataset, the serializer used, and the data\u2019s column names. You can edit the name, description, and model type (for model files) by clicking on \u2018Edit\u2019. You will not be able to edit files that failed the validation process.</p>"},{"location":"user-interface-features/asset-manager-page/#deletion","title":"Deletion","text":"<p>To delete files uploaded, click on its checkbox then the delete icon.</p> <p></p>"},{"location":"user-interface-features/canvas-page/","title":"Canvas Page","text":"<p>The canvas page is where you design your report using report widgets.</p>"},{"location":"user-interface-features/canvas-page/#report-widgets","title":"Report Widgets","text":"<p>The report widgets panel on the left of the screen lists the report widgets installed in your toolkit. They are categorised into accordions, according to the plugin they belong to. Expand each accordion to view the list of widgets it contains and hover over the widget name to view its description. To add a widget to the canvas, simply drag and drop it onto the canvas.</p> <p>(See How to design a customized report &gt; Step 3)</p>"},{"location":"user-interface-features/canvas-page/#canvas-actions","title":"Canvas Actions","text":"<p>Canvas actions allow you to design and create your own multi-paged report.</p> Icon Action Toggle to view the grid lines the widgets will snap to. Delete page Add new page Move page"},{"location":"user-interface-features/canvas-page/#widget-properties","title":"Widget Properties","text":"<p>To help you style your report, widget properties are design tools that allows you to configure the layout, alignment, and colours of widgets.</p>"},{"location":"user-interface-features/canvas-page/#global-variables","title":"Global Variables","text":"<p>Global Variables is a special functionality developed to help ease the process of designing a report from scratch. We understand that official reports usually come with many features that are consistent throughout its pages, such as headers, footnotes, and names of things. You can define these repeated values as Global Variables that you can use within some of the report widgets.</p> <p>(See How to design a customized report &gt; Step 4)</p>"},{"location":"user-interface-features/canvas-page/#tests-to-run-input-blocks","title":"Tests to Run, Input Blocks","text":"<p>Some report widgets come with dependencies on tests to run or additional user inputs (input blocks). As you include various widgets in your report, the Tests to Run and Input Blocks accordions help you to keep track of what you are currently adding to your test suite.</p>"},{"location":"user-interface-features/homepage/","title":"Homepage","text":"<p>The home page provides you with direct navigation to all the essential functions of AI Verify:</p> <ul> <li>Create New Project: Workflow to create a new project to test an AI model and generate report.</li> <li>Models &amp; Data: Management of assets (dataset and model files) uploaded onto the toolkit.</li> <li>Plugins: Management of plugins installed and installation of new plugins.</li> <li>Report Templates: Management of report templates created/installed and creation of new report templates.</li> </ul> <p>On the home page, you will also be able to see the list of all projects you have created. Each project can be identified by its project name and the name of the report template used (if any).</p>"},{"location":"user-interface-features/homepage/#filters","title":"Filters","text":"<p>You can search for a project using the search box or the filters provided.</p> <p>The filters categorise your projects based on the following statuses:</p> Category Description Completed Your project has successfully completed all its tests to run, and a PDF of its report has been generated. No Report Yet No reports have been generated for the project as no tests have been run. Running Test Tests for the project are currently being run. Generating PDF Tests (if any) for the project have completed running and a PDF report is currently being generated."},{"location":"user-interface-features/homepage/#project-actions","title":"Project Actions","text":"Icon Action View generated PDF report. Edit Project Duplicate Project (Copy project to a new project) Delete Project"},{"location":"user-interface-features/input-blocks-page/","title":"Input Blocks Page","text":""},{"location":"user-interface-features/input-blocks-page/#progress","title":"Progress","text":"<p>Some reports can require many tests to run and user inputs. The \u2018Tests Arguments\u2019 and \u2018Input Blocks Progress\u2019 accordions on the left of the page helps you keep track of what is done and what isn\u2019t. You can jump to focus on a particular input block by clicking on its name in the progress accordion.</p>"},{"location":"user-interface-features/input-blocks-page/#file-selection","title":"File Selection","text":"<p>Here is where you select the various files required. By clicking on \u2018Choose dataset/model\u2019 you can select from a list of previously uploaded files or upload a new file to be used.</p>"},{"location":"user-interface-features/input-blocks-page/#test-parameters-input-blocks","title":"Test Parameters &amp; Input Blocks","text":"<p>For each of the tests to run, click on \u2018Open\u2019 to open the dialog box to fill up its required test arguments.</p> <p>For each of the input blocks required, click on \u2018Open\u2019 to open the dialog box to fill up its required inputs.</p>"},{"location":"user-interface-features/plugin-manager-page/","title":"Plugin Manager Page","text":"<p>You can access the Plugin manager via the home page or the menu icon at the header.</p> <p></p>"},{"location":"user-interface-features/plugin-manager-page/#search","title":"Search","text":"<p>In the plugin manager page, you can view the list of plugins installed in your toolkit on the left panel. The toolkit comes with a set of pre-installed plugins, to view more details about these plugins, check out the Plugin Documentation here. Using the search box, you can search for a plugin by:</p> <ul> <li>Name/ description of the plugin</li> <li>Name/ description of the plugin component</li> <li>Tags</li> </ul>"},{"location":"user-interface-features/plugin-manager-page/#filters","title":"Filters","text":"<p>You can sort the list of plugins by its name/ installed date, and filter the plugins shown using the following filters:</p> Filter Description Pre-installed Show plugins that are pre-installed in the toolkit. Templates Show plugins that contain report templates. Widgets Show plugins that contain report widgets. Algorithms Show plugins that contain testing algorithms. Input Blocks Show plugins that contain input blocks. <p>The contents of each plugin are categorised into Widgets, Algorithms, Input Blocks, and Templates. Under each plugin, click on the respective tabs to view its components.</p>"},{"location":"user-interface-features/plugin-manager-page/#required-dependencies-and-packages","title":"Required dependencies and packages","text":"<p>Algorithm-type components come with a list of software packages you will need to have in your machine to run tests using that algorithm. The toolkit does a scan of the packages you have installed in your machine, and checks for its compatibility with the required packages. You are advised to update/install all of the required packages.</p> <p></p>"},{"location":"user-interface-features/plugin-manager-page/#installing-and-updating-plugins","title":"Installing and Updating plugins","text":"<p>Plugins for AI Verify are distributed as zip files. To update a plugin to a newer version, you can simply install the plugin again. To install a plugin, click on \u2018Install\u2019 and select the zip file.</p>"},{"location":"user-interface-features/report-generating-page/","title":"Report Generating Page","text":"<p>In the report generating page, the progress and logs of each technical test being run is displayed. These logs can be used for debugging purposes for failed test runs. To abort any tests being run, click on \u2018Stop\u2019.</p> <p>Once all tests have completed running, a PDF report will be generated and you can view it in your browser by clicking on \u2018View Report\u2019.</p> <p></p>"},{"location":"user-interface-features/report-template-manager-page/","title":"Report Template Manager Page","text":"<p>You can access the report template manager via the home page or the menu icon at the header.</p> <p></p> <p>The report template manager page lists the templates you have installed in the toolkit and any templates that you have created.</p> <p>Each template is identified by its name, description, and author. You can search for a template using the search bar, and sort the list according to their names.</p> <p>To create a new template, click on \u2018Create New Template\u2019</p>"},{"location":"user-interface-features/report-template-manager-page/#template-actions","title":"Template actions","text":"<p>For each template, there are template actions available:</p> Icon Action Edit template. Duplicate template. (Copy template to a new instance) Delete template.* (Projects using the template will not be deleted) <p>*Templates that are installed as part of plugins cannot be deleted.</p>"},{"location":"using-toolkit-step-by-step/design-customized-report/0-intro/","title":"Intro","text":"<p>In this step-by-step tutorial, we will walk you through how you can design a customised report to suit your use case and export it as a Report Template Plugin to be shared with other users.</p> <p>To start, Launch AI Verify (http://localhost:3000/home)</p>"},{"location":"using-toolkit-step-by-step/design-customized-report/1-create-new-project/","title":"1. Create New Project","text":"<p>On the homepage, click on 'Create New Project'.</p> <p></p> <p>Provide the general information:</p> Field Description (required) Project Name This is the name that will help you identify this project within the AI Verify user interface. Project Description This is a short description you can use to briefly describe the scope and objective of this project. E.g. To test whether the XYZ classification model is fair towards all gender groups. Report Title Fill in this field to define the title of the report to be generated. Check \u2018Use Project Name\u2019 to automatically use the project name as the report title. Company Name Fill in the name of the company that owns the AI model being tested. This field is required if you will be using the AI Verify Report Template. <p>You will also be able to use the values entered in these fields in other parts of the report if desired. (See Using Global Variables )</p> <p>Click 'Next' to proceed.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/2-start-empty-canvas/","title":"2. Start Empty Canvas","text":"<p>Click on \u2018Blank Canvas\u2019 to skip using any report templates and build one up from scratch.</p> <p>You can also choose to start with a report and edit/add/remove its widgets to customise it to your needs.</p> <p>Click 'Next'.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/3-design-report-with-widgets/","title":"3. Design Report with Widgets","text":"<p>In this Canvas page, you can design and preview how the report to be generated will look like. In the Report Widgets panel, expand each accordion to explore the report widgets installed. To add the widget to your report, drag and drop it onto the canvas. You can drag to move the widgets anywhere within the page and adjust its dimensions.</p> <p>AI Verify comes with a set of decorator widgets under the plugin \u2018AI Verify Stock Decorators\u2019. Let\u2019s try out the \u2018Header 6\u2019 widget by dragging it onto the canvas.</p> <p></p> <p>To edit the widget, click on the widget on the canvas to highlight it, then click on its edit icon at the top right corner to open the Widget Content editor.</p> <p></p> <p>The \u2018Header 6\u2019 widget contains:</p> Field Description Title Fill up this field to allow the widget to populate the canvas with text styled as \u2018Header 6\u2019. Text Fill up this field to allow the widget to populate the canvas with unstyled text below the styled Title. <p>Click 'Ok' when done.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/4-using-global-variable/","title":"4. Using Global Variables","text":"<p>The project details you previously filled are saved as global variables and can be used as widget content. To use any of the global variables as the widget content, click on \u2018Variable\u2019 and select the fields available.</p> <p></p> <p>You can also set custom global variables by clicking on the \u2018Global Variables\u2019 accordion.</p> <p></p> <p>Fill up the following fields and click on \u2018+\u2019 to create the custom global variable:</p> Field Description Name This is the name to identify the global variable by. Value This is the value to be used when this global variable is selected to be used in a widget. <p></p> <p>Edit a widget and select the Name of the global variable you just created to automatically substitute the widget content field with its value.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/5-add-new-pages/","title":"5. Adding New Pages","text":"<p>To create a new page, click on '+\u2019. You can add a new page after the last page, before the first page, or after a specific page. Click on \u2018Add Page\u2019 to confirm.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/6-using-widget-with-dependencies/","title":"6. Using Widgets with Dependencies","text":"<p>Some widgets have dependencies on tests to run.</p> <p>Let\u2019s include the \u2018ALE Line Graphs\u2019 widget. (You can search for it using the handy search bar in the Report Widgets accordion) This widget is dependent on test results from the Accumulated Local Effect test, and by including it in your report canvas, there is now 1 test to be run.</p> <p></p> <p>Some widgets have dependencies on additional user inputs.</p> <p>Let\u2019s include the \u2018Summary - Safety\u2019 widget. (You can search for it using the handy search bar in the Report Widgets accordion) This widget is dependent on user input from the Safety Process Checklist, and by including it in your report canvas, there is now 1 Input Block to be filled.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/7-save-as-template/","title":"7. Save as Report Template","text":"<p>When you are satisfied with your report canvas, you can save it as a report template locally so that you can use it for future projects. Click on the download icon, then 'Save as Template\u2019.</p> <p></p> <p>The template detail fields are automatically generated from the project details, but you can edit them. Click \u2018Save\u2019 to save the report template.</p> <p></p> <p>Click 'Ok'.</p> <p></p> <p>The report canvas you just designed is now available as a report template on your AI Verify Toolkit. You can further edit it in the future from the Report Templates Page.</p> <p></p>"},{"location":"using-toolkit-step-by-step/design-customized-report/8-export-as-plugin/","title":"8. Export as Plugin","text":"<p>To share a report template with other users, you can export it as a plugin by clicking on the edit button to enter the report template canvas screen.</p> <p></p> <p>Click 'Export as Plugin'.</p> <p></p> <p>The Plugin GID and Template CID fields are automatically generated, but you can edit it to any unique ID that would help you better identify the plugin.</p> <p>Click on \u2018Export\u2019 to download the plugin .zip file.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/0-intro/","title":"Introduction","text":"<p>In this step-by-step tutorial, we will walk you through how you can test your AI Model and generate a report that aligns with the AI Verify Testing Framework.</p> <p>Before starting this tutorial, you will need to download these files:</p> Description AI Model This is a binary classification model that is trained on mock credit risk dataset. Download Here Test Dataset This is a sample tabular dataset which we will be using as the testing dataset, groudn truth dataset, and background dataset. Download Here <p>These sample files are based on a mock Credit Risk use case. We train a binary classification model to predict whether an applicant will default the loan using mock data.</p> <p>If you have not setup and run AI Verify, follow this guide. Then, start AI Verify (http://localhost:3000/home)</p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/1-create-new-project/","title":"1. Create New Project","text":"<p>On the homepage, click on 'Create New Project'</p> <p></p> <p>Provide the general information as shown in the table below:</p> Field Description Value (required) Project Name To identify this project in the home page Testing the credit model Project Description To describe the scope and objective of this project E.g. To test whether the XYZ classification model is fair towards all gender groups. To test how the credit model aligns with the AI Verify Testing Framework Report Title To define the title of the report to be generated. Check \u2018Use Project Name\u2019 to automatically use the project name as the report title. Testing the credit model Company Name To fill the name of the company that owns the AI model being tested.   This field is required if you are using the AI Verify Report Template. Fake Company Pte Ltd <p>You will also be able to use the values entered in these fields in other parts of the report if desired. (See \u2018How to design a customised report &gt; Using Global Variables\u2019 )</p> <p>Click 'Next' to proceed.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/10-run-test-and-generate-report/","title":"10. Run Test and Generate Report","text":"<p>Before proceeding to run the tests and generate the report, you can check for the completion of required test arguments and process checks under \u2018Test Arguments\u2019 and \u2018Input Blocks Progress\u2019 accordion. The widgets in the AI Verify Summary Report Template for Classification Models does not allow for report generation if any of the required components are incomplete.</p> <p>Once verified that all test arguments and process checks are completed, click on \u2018Next\u2019.</p> <p></p> <p>Click 'Proceed'.</p> <p></p> <p>Now, sit back and watch as the AI Verify Test Engine does its magic!</p> <p>Once the tests completed running, view PDF report generated by clicking on \u2018View Report\u2019.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/2-select-template/","title":"2. Select a Report Template","text":"<p>Click to select the report template to use.</p> <p>For this tutorial, select AI Verify Summary Report Template for Classification Models.</p> <p>Once a template is selected for a project, it cannot be reverted. To change a template, a new project will need to be created.</p> <p>Click 'Next' to proceed to the Canvas page.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/3-design-the-report/","title":"3. Design The Report","text":"<p>In this Canvas page, you can design and preview how the report to be generated will look like. Since the AI Verify Summary Report Template for Classification Models is used for this project, the canvas is already populated with report components (widgets) that will help you obtain a report that aligns with the AI Verify Testing Framework.</p> <p>To edit the Report Title and Company Name, click on the cover page widget on Page 1 and the edit icon at the top right corner of the outlined widget to open the Widget Content editor.</p> <p></p> <p></p> <p>To navigate through the report pages, click on the page numbers or enter the page number to navigate to a specific page.</p> <p></p> <p>In this tutorial, we will not be adding or removing any widgets from the canvas.</p> <p>Click 'Next' to proceed to the Input Blocks page.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/4-select-testing-dataset/","title":"4. Select Testing Dataset","text":"<p>The AI Verify Report Template includes components that require technical tests to be run. To run these tests, you will need to upload the Testing Dataset, Ground Truth Dataset, and AI Model.</p> <p> Click on \u2018Choose Dataset\u2019.</p> <p></p> <p>If you have previously uploaded the dataset to be used, click on the row of the dataset to be used. Note that datasets marked \u2018Invalid\u2019 have invalid properties and cannot be used. </p> <p>Click on \u2018Use Dataset\u2019 and Skip to Step 5.</p> <p>To upload new datasets, click on \u2018New Dataset +\u2019.</p> <p></p> <p>Drag and drop the dataset file(s) onto the drop box or click to select files. A maximum of 10 files can be uploaded at once. You should also take the chance to upload the Ground Truth and Background Datasets as well. For image datasets, you can upload a folder containing the image files by clicking on \u2018Upload Folder\u2019 to select the folder to be uploaded. For more information on dataset preparation, (See Getting Started &gt; Preparation of Input Files)</p> <p>Click \u2018Upload Selected Files &gt;\u2019.</p> <p></p> <p>Once dataset validation is completed, you can view the dataset information on the right panel. If the dataset is valid, you can edit the dataset name and description by clicking on \u2018Edit\u2019. If the dataset is invalid, refer to the error message for more information.</p> <p>Click on \u2018Back to all datasets\u2019.</p> <p></p> <p>Select the Testing Dataset to be used by clicking on its row, then \u2018Use Dataset\u2019.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/5-select-ground-truth-dataset/","title":"5. Select Ground Truth Dataset","text":"<p>Click on \u2018Choose Dataset\u2019. Click on the row to select the dataset to be used as ground truth. The testing dataset can be used if it contains the ground truth. </p> <p>Click on \u2018Use Dataset\u2019.</p> <p></p> <p>Click on the dropdown. Select \"default\" as the ground truth column.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/6-select-ai-model/","title":"6. Select AI Model","text":"<p>Click on \u2018Choose Model\u2019. If you have previously uploaded the AI Model to be tested, click on the row of the AI Model. Note that AI Models marked \u2018Invalid\u2019 have invalid properties and cannot be used.</p> <p>Click on \u2018Use Model\u2019 and Skip to Step 7</p> <p>To upload new AI Models, click on \u2018New Model +\u2019.</p> <p></p> <p>Select the type of AI Model to upload (AI Model / Pipeline) and click \u2018Next\u2019. For this tutorial, we will be uploading an AI Model.</p> <p>Drag and drop the AI Model file(s) onto the drop box or click to select files. A maximum of 10 files can be uploaded at once. For TensorFlow models, click on \u2018Upload Folder\u2019 to select the folder to be uploaded. For more information on model preparation, (See Getting Started &gt; Preparation of Input Files).</p> <p>Click on the dropdown beside each file picked to indicate its model type. In this tutorial, we will use Classification.</p> <p>Click \u2018Upload Selected Files &gt;\u2019.</p> <p></p> <p>Once model validation is completed, you can view the model information on the right panel.</p> <p>If the model is valid, you can edit the model name, description and model type by clicking on \u2018Edit\u2019. You should describe the model's purpose as its description if you are using the AI Verify report templates.</p> <p>If the model is invalid, refer to the error message for more information.</p> <p>Click on \u2018Back to all Models\u2019.</p> <p></p> <p></p> <p>Select the AI Model to be tested by clicking on its row, then \u2018Use Model\u2019.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/7-provide-test-args/","title":"7. Provide Test Arguments","text":"<p>The AI Verify Summary Report Template for Classification Models requires Fairness (Fairness Metrics Toolbox for Classification), Robustness (Robustness Toolbox) and Explainability (SHAP Toolbox) tests to be run. These tests require configuration of some test arguments in addition to the Datasets and AI Model files uploaded. For more information on these tests (see References &gt; List of AI Verify Plugins).</p> <p></p> <p>Click on \u2018Open\u2019 for Fairness Metrics Toolbox for Classification. Provide the test argument:</p> Test Argument Description Value (required) Sensitive Feature Names: This is the column name of the sensitive feature in the testing dataset. gender, race <p>If there are more than one sensitive feature, click on \u2018+\u2019.</p> <p>Click on \u2018Ok\u2019.</p> <p></p> <p>Click on \u2018Open\u2019 for Robustness Toolbox.</p> <p>Provide the test arguments:</p> Test Argument Description Value Annotated ground truth path* This is the path to the annotated ground truth file for image datasets. (See Getting Started &gt; Preparation of Input Files) Select the path to testing dataset Name of column containing image file names* This is the column name in the annotated ground truth file for image datasets that contains the image file names. file_name <p>*This argument is required only if you are testing an image dataset. For tabular datasets like the one used in this tutorial, it can be left blank.</p> <p>Click on \u2018Ok\u2019.</p> <p></p> <p>Click on \u2018Open\u2019 for SHAP Toolbox.</p> <p>Provide the test arguments:</p> Test Argument Description Value (required) Type of Explainability This is the type of explainability test to be run. Select \"global\" (required) Path of the Background This is the path to the background dataset to be used to create permutations. (See Getting Started &gt; Preparation of Input Files) Select the path to testing dataset (required) Size of the Background* This is the number of data points from the background dataset to be sampled. Enter 0 to skip sampling and use the entire dataset. 100 (required) Type of the Test Dataset* This is the number of data points from the test dataset to be sampled. Enter 0 to skip sampling and use the entire dataset. 100 <p>*Recommended number of test data points is 100, selecting a larger sample will require a longer computation time.</p> <p>Click on 'Ok'.</p> <p></p> <p>The completion of required Test Arguments can be tracked in the progress accordion. Tests with invalid arguments will not be able to run.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/8-complete-process-checklist/","title":"8. Complete Process Checklist","text":"<p>The AI Verify Summary Report Template for Classification Models requires process checklists for all 11 AI ethics principles to be completed:</p> AI Ethics Principles No. of Checks Transparency 8 Explainability 1 Reproducibility 15 Safety 9 Robustness 7 Fairness 10 Accountability 8 Human Agency &amp; Oversight 8 Security 14 Data Governance Process Checklist 4 Inclusve Growth, Societal &amp; Environmental Well-being 1 <p>Download the testing framework</p> <p>Click on \u2018Open\u2019.</p> <p></p> <p>Complete each testable criteria by indicating its criterion completion status:</p> <ul> <li>Yes: The criterion is implemented with documentary evidence.</li> <li>No: The criterion has not been implemented.</li> <li>Not Applicable: The criterion is not relevant to the use case.</li> </ul> <p>You can provide relevant elaboration as such:</p> <ul> <li>If Yes, describe how the process checks is implemented and/or how the   evidence has been documented. (where applicable)</li> <li>If No, state the reason(s) for not implementing the process checks or not documenting evidence.</li> <li>If Not Applicable, state reason(s)</li> </ul> <p></p> <p>For each AI ethic principle, you can provide a summarized justification which will be displayed in the Summary Report segment of the report generated like this: </p> <p>Click on \u2018Ok\u2019 once all checks are complete.</p> <p></p> <p>Complete the rest of the Process Checklists. You can easily view your input block completion progress in the accordion or within the Input Block for the specific process checklist.</p> <p></p>"},{"location":"using-toolkit-step-by-step/test-ai-model-and-generate-report/9-select-fairness-metrics-using-fainess-tree/","title":"9. Select Fairness Metrics using Fairness Tree","text":"<p>The AI Verify Summary Report Template for Classification Models contains a decision tree component that aids users to select the most appropriate fairness metric for their use case. The Fairness Tree will be contextualised to your AI use case based on the definitions you provide.</p> <p>Click on \u2018Open\u2019.</p> <p></p> <p>Provide the following definitions:</p> Definition Description Value (required) Sensitive Feature Name(s) This is the sensitive feature(s) previously selected in Step 7. Use commas to separate the features (e.g. gender, race) gender, race (required) Favourable Allocated Resource / Opportunity This is the resource/ opportunity that is favourable in this use case. (e.g. bail, loan, low interest rate, low insurance premium) low interest rate (required) Qualified Group This is the name of the group that is supposed to receive the resource / opportunity. (e.g. non-reoffenders, qualified applicants, licensed companies) qualified applicants (required) Unqualified Group This is the name of the group that is NOT supposed to receive the resource / opportunity. (e.g. reoffenders, unqualified applicants, unlicensed companies) unqualified applicants <p>Click \u2018Next\u2019 once all fields are filled.</p> <p></p> <p>The first level of the fairness tree asks to consider what is fair for your use case. Select all of the desired outcomes applicable. A minimum of 1 and maximum of 3 outcomes can be selected. For each desired outcome selected, there are guiding questions to guide you in documenting the reasons for your selection. </p> <p>Click \u2018Next\u2019 once completed.</p> <p></p> <p>Select the relevant options and document the reasons for the third and final level of the decision tree.</p> <p>Click \u2018Next\u2019 once completed.</p> <p></p> <p>With the options selected, the fairness tree has selected the most appropriate fairness metric(s) for your use case. You can scroll to zoom in to further inspect the contextualized decision tree. To reset the metric selection, click on \u2018Reset Graph\u2019.</p> <p>If you are satisfied with the metric(s) selected, click \u2018Ok\u2019.</p> <p></p>"}]}