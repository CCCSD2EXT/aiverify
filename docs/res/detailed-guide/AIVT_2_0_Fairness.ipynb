{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OI-LA_bGEOr"
      },
      "source": [
        "# PyPi package reference\n",
        "\n",
        "Fairness Metrics Toolbox for Classification\n",
        "https://pypi.org/project/aiverify-fairness-metrics-toolbox-for-classification/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwBTGipb6wmG"
      },
      "source": [
        "# Install package, download data and model files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kngyPHky4l9K",
        "outputId": "a6679fde-50c1-4468-a13c-e927b1733255"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "google-genai 1.11.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.26.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install aiverify-fairness-metrics-toolbox-for-classification --quiet\n",
        "\n",
        "!wget -q https://github.com/aiverify-foundation/aiverify/blob/e2a0099bf51837e516ef09ca7115cbcbd5d8896c/stock-plugins/aiverify.stock.fairness-metrics-toolbox-for-classification/algorithms/fairness_metrics_toolbox_for_classification/tests/user_defined_files/data/sample_mc_toxic_data.sav\n",
        "!wget -q https://github.com/aiverify-foundation/aiverify/blob/e2a0099bf51837e516ef09ca7115cbcbd5d8896c/stock-plugins/aiverify.stock.fairness-metrics-toolbox-for-classification/algorithms/fairness_metrics_toolbox_for_classification/tests/user_defined_files/model/sample_mc_toxic_sklearn_linear.LogisticRegression.sav"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obeWTxWv6zge"
      },
      "source": [
        "## Execute Fairness test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aadd951pDTKa"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38HDAafxGNf9"
      },
      "source": [
        "Execute the python module by providing necessary arguments including data path, model path, ground truth path, ground truth field and model type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzOQUGsd5fTU"
      },
      "outputs": [],
      "source": [
        "!python -m aiverify_fairness_metrics_toolbox_for_classification \\\n",
        "--data_path \"sample_mc_toxic_data.sav\" \\\n",
        "--model_path \"sample_mc_toxic_sklearn_linear.LogisticRegression.sav\" \\\n",
        "--ground_truth_path \"sample_mc_toxic_data.sav\" \\\n",
        "--ground_truth toxic \\\n",
        "--model_type CLASSIFICATION \\\n",
        "--sensitive_features_list gender"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9kJDzd754Er",
        "outputId": "9e9b28a1-7d52-4764-df10-6281691d486a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample_data\t\t  sample_mc_toxic_sklearn_linear.LogisticRegression.sav\n",
            "sample_mc_toxic_data.sav  test.log\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m54u3V5lGaGq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8qN8q4sGasK"
      },
      "source": [
        "Generated result will be stored in `results.json` file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-tMSkwE6DeT",
        "outputId": "0de10eaf-6c03-4f67-a801-ff8c71790a3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cat: /content/output/results.json: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cat /content/output/results.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4isVQXTGePU"
      },
      "source": [
        "Upload the `result.json` file to the Portal to generate report if needed. [Follow the detailed guide here for reference](https://aiverify-foundation.github.io/aiverify/detailed-guide/fairness-test/)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
